\section{Introduction}

As large language models (LLMs) began to see serious commercial adoption around 2023, interest in the concept of Retrieval Augmented Generation (RAG) grew alongside them. This period marked one of the first moments when LLMs moved beyond simply imitating existing styles—such as writing poems—or serving as somewhat limited conversational partners, and instead led companies to believe that these models could be applied in genuinely useful, real-world scenarios.

Following the release of ChatGPT, people—particularly those in industry—started to continuously explore how this technology might contribute to creating business value. Encountering well-trained (pretrained) LLMs, many were impressed, and at times even unsettled, by responses that felt strikingly ``human''. Yet this impression did not last long before the limitations became clear. Models lack awareness of information beyond their training cutoff, cannot access internal documents or organizational policies, and have no inherent understanding of the most recent data. On top of that, they frequently suffer from hallucinations, generating outputs that sound convincing but are not factually correct.

From a corporate perspective, these shortcomings are more than academic concerns—they are critical constraints. For deployment in real business environments, systems must ensure accuracy, timeliness, and the reliability of their information sources. A model that merely ``sounds articulate'' carries significant risk when used for customer-facing applications or internal decision-support tools. Inevitably, this led to a fundamental question: How can we safely make use of information that the model itself does not know?

Retrieval Augmented Generation (RAG) emerged as a practical and compelling answer. At its core, RAG is built on a simple idea: instead of forcing the model to internalize all possible knowledge during training, retrieve the relevant external information at inference time and supply it alongside the prompt. In this setup, the LLM is not treated as a vast knowledge store, but rather as a reasoning and language-generation engine, while factual knowledge and data are managed by external systems.

In the following sections, we will examine the key components of a RAG system—document collection and preprocessing, embeddings and vector stores, retrieval strategies, and prompt design during generation. Along the way, we will explore why this architecture works so well in practice, and where its limitations and failure modes tend to appear.